{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Networks (ANN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that we are about to deal with is a classification problem.\n",
    "\n",
    "We have several independent variables, like credit score, the balance, the number of products...\n",
    "\n",
    "and based on these independent variables,\n",
    "\n",
    "we are trying to predict which customers are leaving the bank.\n",
    "\n",
    "ANN can do a terrific job at doing this,\n",
    "\n",
    "and making that kind of predictions..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theano Libray\n",
    "\n",
    "Theano is an open source numerical computations library,\n",
    "\n",
    "very efficient for fast numerical computations.\n",
    "\n",
    "And that is based on numpy syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Library\n",
    "\n",
    "Tensorflow is another numerical computations library\n",
    "\n",
    "that runs very fast computations.\n",
    "\n",
    "And that can run our CPU or GPU\n",
    "\n",
    "CPU : Central Processing Unit\n",
    "\n",
    "GPU : Graphical Processing Unit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras Library\n",
    "\n",
    "The Keras library is an amazing library to build deep learning models,\n",
    "\n",
    "in a few lines of code.\n",
    "\n",
    "Keras is a library based on Theano and Tensorflow,\n",
    "\n",
    "and exactly as we use scikit-learn to build very efficiently machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "\n",
    "dataset = pd.read_csv('Churn_Modelling.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "the key thing to understand here is that\n",
    "all these variables here are independent variables.\n",
    "but the last columns is our dependen variable.\n",
    "1 : exited, 0 : stayed.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RowNumber</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>Surname</th>\n",
       "      <th>CreditScore</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Age</th>\n",
       "      <th>Tenure</th>\n",
       "      <th>Balance</th>\n",
       "      <th>NumOfProducts</th>\n",
       "      <th>HasCrCard</th>\n",
       "      <th>IsActiveMember</th>\n",
       "      <th>EstimatedSalary</th>\n",
       "      <th>Exited</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>15634602</td>\n",
       "      <td>Hargrave</td>\n",
       "      <td>619</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>101348.88</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>15647311</td>\n",
       "      <td>Hill</td>\n",
       "      <td>608</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>41</td>\n",
       "      <td>1</td>\n",
       "      <td>83807.86</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>112542.58</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>15619304</td>\n",
       "      <td>Onio</td>\n",
       "      <td>502</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>42</td>\n",
       "      <td>8</td>\n",
       "      <td>159660.80</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113931.57</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>15701354</td>\n",
       "      <td>Boni</td>\n",
       "      <td>699</td>\n",
       "      <td>France</td>\n",
       "      <td>Female</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>93826.63</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>15737888</td>\n",
       "      <td>Mitchell</td>\n",
       "      <td>850</td>\n",
       "      <td>Spain</td>\n",
       "      <td>Female</td>\n",
       "      <td>43</td>\n",
       "      <td>2</td>\n",
       "      <td>125510.82</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79084.10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n",
       "0          1    15634602  Hargrave          619    France  Female   42   \n",
       "1          2    15647311      Hill          608     Spain  Female   41   \n",
       "2          3    15619304      Onio          502    France  Female   42   \n",
       "3          4    15701354      Boni          699    France  Female   39   \n",
       "4          5    15737888  Mitchell          850     Spain  Female   43   \n",
       "\n",
       "   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n",
       "0       2       0.00              1          1               1   \n",
       "1       1   83807.86              1          0               1   \n",
       "2       8  159660.80              3          1               0   \n",
       "3       1       0.00              2          0               0   \n",
       "4       2  125510.82              1          1               1   \n",
       "\n",
       "   EstimatedSalary  Exited  \n",
       "0        101348.88       1  \n",
       "1        112542.58       0  \n",
       "2        113931.57       1  \n",
       "3         93826.63       0  \n",
       "4         79084.10       0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "\n",
    "y = dataset.iloc[:, 13:14].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 'France', 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "       [608, 'Spain', 'Female', 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "       [502, 'France', 'Female', 42, 8, 159660.8, 3, 1, 0, 113931.57],\n",
       "       [699, 'France', 'Female', 39, 1, 0.0, 2, 0, 0, 93826.63],\n",
       "       [850, 'Spain', 'Female', 43, 2, 125510.82, 1, 1, 1, 79084.1],\n",
       "       [645, 'Spain', 'Male', 44, 8, 113755.78, 2, 1, 0, 149756.71],\n",
       "       [822, 'France', 'Male', 50, 7, 0.0, 2, 1, 1, 10062.8],\n",
       "       [376, 'Germany', 'Female', 29, 4, 115046.74, 4, 1, 0, 119346.88],\n",
       "       [501, 'France', 'Male', 44, 4, 142051.07, 2, 0, 1, 74940.5],\n",
       "       [684, 'France', 'Male', 27, 2, 134603.88, 1, 1, 1, 71725.73]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [1],\n",
       "       [0],\n",
       "       [0]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "# Encoding the Independent Variable\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 0, 'Female', 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "       [608, 2, 'Female', 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "       [502, 0, 'Female', 42, 8, 159660.8, 3, 1, 0, 113931.57],\n",
       "       [699, 0, 'Female', 39, 1, 0.0, 2, 0, 0, 93826.63],\n",
       "       [850, 2, 'Female', 43, 2, 125510.82, 1, 1, 1, 79084.1],\n",
       "       [645, 2, 'Male', 44, 8, 113755.78, 2, 1, 0, 149756.71],\n",
       "       [822, 0, 'Male', 50, 7, 0.0, 2, 1, 1, 10062.8],\n",
       "       [376, 1, 'Female', 29, 4, 115046.74, 4, 1, 0, 119346.88],\n",
       "       [501, 0, 'Male', 44, 4, 142051.07, 2, 0, 1, 74940.5],\n",
       "       [684, 0, 'Male', 27, 2, 134603.88, 1, 1, 1, 71725.73]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_X1 = LabelEncoder()\n",
    "\n",
    "X[:, 1]         = labelencoder_X1.fit_transform(X[:, 1])\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[619, 0, 0, 42, 2, 0.0, 1, 1, 1, 101348.88],\n",
       "       [608, 2, 0, 41, 1, 83807.86, 1, 0, 1, 112542.58],\n",
       "       [502, 0, 0, 42, 8, 159660.8, 3, 1, 0, 113931.57],\n",
       "       [699, 0, 0, 39, 1, 0.0, 2, 0, 0, 93826.63],\n",
       "       [850, 2, 0, 43, 2, 125510.82, 1, 1, 1, 79084.1],\n",
       "       [645, 2, 1, 44, 8, 113755.78, 2, 1, 0, 149756.71],\n",
       "       [822, 0, 1, 50, 7, 0.0, 2, 1, 1, 10062.8],\n",
       "       [376, 1, 0, 29, 4, 115046.74, 4, 1, 0, 119346.88],\n",
       "       [501, 0, 1, 44, 4, 142051.07, 2, 0, 1, 74940.5],\n",
       "       [684, 0, 1, 27, 2, 134603.88, 1, 1, 1, 71725.73]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelencoder_X2 = LabelEncoder()\n",
    "\n",
    "X[:, 2]         = labelencoder_X2.fit_transform(X[:, 2])\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sefa3\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:415: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "C:\\Users\\Sefa3\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:451: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.1900000e+02,\n",
       "        0.0000000e+00, 4.2000000e+01, 2.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0134888e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 6.0800000e+02,\n",
       "        0.0000000e+00, 4.1000000e+01, 1.0000000e+00, 8.3807860e+04,\n",
       "        1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.1254258e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.0200000e+02,\n",
       "        0.0000000e+00, 4.2000000e+01, 8.0000000e+00, 1.5966080e+05,\n",
       "        3.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.1393157e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.9900000e+02,\n",
       "        0.0000000e+00, 3.9000000e+01, 1.0000000e+00, 0.0000000e+00,\n",
       "        2.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.3826630e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 8.5000000e+02,\n",
       "        0.0000000e+00, 4.3000000e+01, 2.0000000e+00, 1.2551082e+05,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 7.9084100e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 6.4500000e+02,\n",
       "        1.0000000e+00, 4.4000000e+01, 8.0000000e+00, 1.1375578e+05,\n",
       "        2.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.4975671e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 8.2200000e+02,\n",
       "        1.0000000e+00, 5.0000000e+01, 7.0000000e+00, 0.0000000e+00,\n",
       "        2.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0062800e+04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 3.7600000e+02,\n",
       "        0.0000000e+00, 2.9000000e+01, 4.0000000e+00, 1.1504674e+05,\n",
       "        4.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.1934688e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.0100000e+02,\n",
       "        1.0000000e+00, 4.4000000e+01, 4.0000000e+00, 1.4205107e+05,\n",
       "        2.0000000e+00, 0.0000000e+00, 1.0000000e+00, 7.4940500e+04],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.8400000e+02,\n",
       "        1.0000000e+00, 2.7000000e+01, 2.0000000e+00, 1.3460388e+05,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 7.1725730e+04]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to create dummy variables\n",
    "\n",
    "onehotencoder   = OneHotEncoder(categorical_features = [1])\n",
    "\n",
    "X               = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.1900000e+02,\n",
       "        0.0000000e+00, 4.2000000e+01, 2.0000000e+00, 0.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0134888e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 6.0800000e+02,\n",
       "        0.0000000e+00, 4.1000000e+01, 1.0000000e+00, 8.3807860e+04,\n",
       "        1.0000000e+00, 0.0000000e+00, 1.0000000e+00, 1.1254258e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.0200000e+02,\n",
       "        0.0000000e+00, 4.2000000e+01, 8.0000000e+00, 1.5966080e+05,\n",
       "        3.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.1393157e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.9900000e+02,\n",
       "        0.0000000e+00, 3.9000000e+01, 1.0000000e+00, 0.0000000e+00,\n",
       "        2.0000000e+00, 0.0000000e+00, 0.0000000e+00, 9.3826630e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 8.5000000e+02,\n",
       "        0.0000000e+00, 4.3000000e+01, 2.0000000e+00, 1.2551082e+05,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 7.9084100e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 1.0000000e+00, 6.4500000e+02,\n",
       "        1.0000000e+00, 4.4000000e+01, 8.0000000e+00, 1.1375578e+05,\n",
       "        2.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.4975671e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 8.2200000e+02,\n",
       "        1.0000000e+00, 5.0000000e+01, 7.0000000e+00, 0.0000000e+00,\n",
       "        2.0000000e+00, 1.0000000e+00, 1.0000000e+00, 1.0062800e+04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 0.0000000e+00, 3.7600000e+02,\n",
       "        0.0000000e+00, 2.9000000e+01, 4.0000000e+00, 1.1504674e+05,\n",
       "        4.0000000e+00, 1.0000000e+00, 0.0000000e+00, 1.1934688e+05],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 5.0100000e+02,\n",
       "        1.0000000e+00, 4.4000000e+01, 4.0000000e+00, 1.4205107e+05,\n",
       "        2.0000000e+00, 0.0000000e+00, 1.0000000e+00, 7.4940500e+04],\n",
       "       [1.0000000e+00, 0.0000000e+00, 0.0000000e+00, 6.8400000e+02,\n",
       "        1.0000000e+00, 2.7000000e+01, 2.0000000e+00, 1.3460388e+05,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0000000e+00, 7.1725730e+04]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first 3 variables : dummy variables\n",
    "\n",
    "X = X.astype('float64')\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 6.1900000e+02, 0.0000000e+00,\n",
       "        4.2000000e+01, 2.0000000e+00, 0.0000000e+00, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0134888e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 6.0800000e+02, 0.0000000e+00,\n",
       "        4.1000000e+01, 1.0000000e+00, 8.3807860e+04, 1.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 1.1254258e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 5.0200000e+02, 0.0000000e+00,\n",
       "        4.2000000e+01, 8.0000000e+00, 1.5966080e+05, 3.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 1.1393157e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 6.9900000e+02, 0.0000000e+00,\n",
       "        3.9000000e+01, 1.0000000e+00, 0.0000000e+00, 2.0000000e+00,\n",
       "        0.0000000e+00, 0.0000000e+00, 9.3826630e+04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 8.5000000e+02, 0.0000000e+00,\n",
       "        4.3000000e+01, 2.0000000e+00, 1.2551082e+05, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 7.9084100e+04],\n",
       "       [0.0000000e+00, 1.0000000e+00, 6.4500000e+02, 1.0000000e+00,\n",
       "        4.4000000e+01, 8.0000000e+00, 1.1375578e+05, 2.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 1.4975671e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 8.2200000e+02, 1.0000000e+00,\n",
       "        5.0000000e+01, 7.0000000e+00, 0.0000000e+00, 2.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 1.0062800e+04],\n",
       "       [1.0000000e+00, 0.0000000e+00, 3.7600000e+02, 0.0000000e+00,\n",
       "        2.9000000e+01, 4.0000000e+00, 1.1504674e+05, 4.0000000e+00,\n",
       "        1.0000000e+00, 0.0000000e+00, 1.1934688e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 5.0100000e+02, 1.0000000e+00,\n",
       "        4.4000000e+01, 4.0000000e+00, 1.4205107e+05, 2.0000000e+00,\n",
       "        0.0000000e+00, 1.0000000e+00, 7.4940500e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 6.8400000e+02, 1.0000000e+00,\n",
       "        2.7000000e+01, 2.0000000e+00, 1.3460388e+05, 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0000000e+00, 7.1725730e+04]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to avoid dummy variable trap,\n",
    "# we need to drop the first column.\n",
    "\n",
    "X = X[:, 1:]\n",
    "\n",
    "X[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "\n",
    "# we need to apply feature scaling\n",
    "# to ease all these calculations.\n",
    "# because, we don't want to have one independent variable\n",
    "# dominating another one.\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc      = StandardScaler()\n",
    "\n",
    "X_train = sc.fit_transform(X_train)\n",
    "\n",
    "X_test  = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.5698444 ,  1.74309049,  0.16958176, -1.09168714, -0.46460796,\n",
       "         0.00666099, -1.21571749,  0.8095029 ,  0.64259497, -1.03227043,\n",
       "         1.10643166],\n",
       "       [ 1.75486502, -0.57369368, -2.30455945,  0.91601335,  0.30102557,\n",
       "        -1.37744033, -0.00631193, -0.92159124,  0.64259497,  0.9687384 ,\n",
       "        -0.74866447],\n",
       "       [-0.5698444 , -0.57369368, -1.19119591, -1.09168714, -0.94312892,\n",
       "        -1.031415  ,  0.57993469, -0.92159124,  0.64259497, -1.03227043,\n",
       "         1.48533467],\n",
       "       [-0.5698444 ,  1.74309049,  0.03556578,  0.91601335,  0.10961719,\n",
       "         0.00666099,  0.47312769, -0.92159124,  0.64259497, -1.03227043,\n",
       "         1.27652776],\n",
       "       [-0.5698444 ,  1.74309049,  2.05611444, -1.09168714,  1.73658844,\n",
       "         1.04473698,  0.8101927 ,  0.8095029 ,  0.64259497,  0.9687384 ,\n",
       "         0.55837842],\n",
       "       [ 1.75486502, -0.57369368,  1.29325423, -1.09168714, -0.17749539,\n",
       "        -1.031415  ,  0.44253504,  0.8095029 ,  0.64259497, -1.03227043,\n",
       "         1.63252134],\n",
       "       [-0.5698444 , -0.57369368,  1.6128308 ,  0.91601335,  0.77954653,\n",
       "        -1.37744033,  0.30432823, -0.92159124, -1.55619021, -1.03227043,\n",
       "         0.48149647],\n",
       "       [-0.5698444 ,  1.74309049, -0.54173384,  0.91601335,  0.20532138,\n",
       "         1.04473698, -1.21571749,  0.8095029 ,  0.64259497,  0.9687384 ,\n",
       "         1.07382167],\n",
       "       [-0.5698444 ,  1.74309049, -0.14999481,  0.91601335,  3.55496808,\n",
       "         1.39076231,  0.80633029, -0.92159124,  0.64259497,  0.9687384 ,\n",
       "        -1.04949755],\n",
       "       [-0.5698444 , -0.57369368, -0.29431972, -1.09168714, -0.65601634,\n",
       "         0.35268632,  1.48636403,  0.8095029 ,  0.64259497, -1.03227043,\n",
       "         0.01539358]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.75486502, -0.57369368, -0.55204276, -1.09168714, -0.36890377,\n",
       "         1.04473698,  0.8793029 , -0.92159124,  0.64259497,  0.9687384 ,\n",
       "         1.61085707],\n",
       "       [-0.5698444 , -0.57369368, -1.31490297, -1.09168714,  0.10961719,\n",
       "        -1.031415  ,  0.42972196, -0.92159124,  0.64259497, -1.03227043,\n",
       "         0.49587037],\n",
       "       [-0.5698444 ,  1.74309049,  0.57162971, -1.09168714,  0.30102557,\n",
       "         1.04473698,  0.30858264, -0.92159124,  0.64259497,  0.9687384 ,\n",
       "        -0.42478674],\n",
       "       [-0.5698444 , -0.57369368,  1.41696129,  0.91601335, -0.65601634,\n",
       "        -0.33936434,  0.57533623, -0.92159124, -1.55619021, -1.03227043,\n",
       "        -0.18777657],\n",
       "       [ 1.75486502, -0.57369368,  0.57162971,  0.91601335, -0.08179119,\n",
       "         0.00666099,  1.38961097,  0.8095029 ,  0.64259497,  0.9687384 ,\n",
       "         0.61684179],\n",
       "       [-0.5698444 ,  1.74309049,  0.20050853, -1.09168714,  1.73658844,\n",
       "        -0.68538967,  1.5900207 ,  0.8095029 ,  0.64259497, -1.03227043,\n",
       "        -0.01930192],\n",
       "       [-0.5698444 ,  1.74309049, -0.62420521,  0.91601335, -0.46460796,\n",
       "        -1.72346566, -0.1640232 ,  0.8095029 , -1.55619021,  0.9687384 ,\n",
       "         1.04587113],\n",
       "       [-0.5698444 ,  1.74309049, -0.14999481, -1.09168714, -0.94312892,\n",
       "         0.35268632,  1.30238547,  0.8095029 ,  0.64259497,  0.9687384 ,\n",
       "         0.01616599],\n",
       "       [-0.5698444 , -0.57369368, -0.54173384, -1.09168714,  2.40651778,\n",
       "         1.39076231, -1.21571749, -0.92159124, -1.55619021,  0.9687384 ,\n",
       "        -1.51196958],\n",
       "       [-0.5698444 , -0.57369368, -2.00560072, -1.09168714,  2.31081359,\n",
       "        -1.37744033,  1.42661775,  0.8095029 , -1.55619021, -1.03227043,\n",
       "         0.70541249]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Now let's make the ANN!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing the Keras libraries and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential # to initialize our neural network.\n",
    "\n",
    "from keras.layers import Dense  # this is the model will use to create layers in our ANN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initializing the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0000000e+00, 0.0000000e+00, 6.1900000e+02, ..., 1.0000000e+00,\n",
       "        1.0000000e+00, 1.0134888e+05],\n",
       "       [0.0000000e+00, 1.0000000e+00, 6.0800000e+02, ..., 0.0000000e+00,\n",
       "        1.0000000e+00, 1.1254258e+05],\n",
       "       [0.0000000e+00, 0.0000000e+00, 5.0200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 1.1393157e+05],\n",
       "       ...,\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.0900000e+02, ..., 0.0000000e+00,\n",
       "        1.0000000e+00, 4.2085580e+04],\n",
       "       [1.0000000e+00, 0.0000000e+00, 7.7200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 9.2888520e+04],\n",
       "       [0.0000000e+00, 0.0000000e+00, 7.9200000e+02, ..., 1.0000000e+00,\n",
       "        0.0000000e+00, 3.8190780e+04]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Adding the input layer and the first hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(1, input_shape = (11, )))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Adding the second hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.add(Dense(6, activation = 'relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Adding the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we have 2 different categories for output,\n",
    "# than we can use the sigmoid function.\n",
    "# if we had more than 2 categories for output,\n",
    "# we'd need to use the softmax function.\n",
    "\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compiling the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Sefa3\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "classifier.compile(optimizer = 'adam', # for stochastic gradient descent algorithm\n",
    "                   loss = 'binary_crossentropy', # to find optimal weights(logarithmic loss)\n",
    "                   metrics = ['accuracy']\n",
    "                  )\n",
    "\n",
    "# loss = 'categorical_crossentropy' for loss, if we have more than 2 categories in the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Fitting the ANN to the Training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 : Making the predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Predicting the Test set results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [False],\n",
       "       [False],\n",
       "       [ True],\n",
       "       [ True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "y_pred = (y_pred > 0.5)\n",
    "\n",
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Making the Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy = (cm[0, 0] + cm[1, 1]) / np.sum(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.801"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : Predicting a single new observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Predict if the customer with the following information will leave the bank:\n",
    "    Geography           : France = [0, 0] <-- corresponds to\n",
    "    Credit Score        : 600\n",
    "    Gender              : Male   = [1]    <-- corresponds to\n",
    "    Age                 : 40\n",
    "    Tenure              : 3\n",
    "    Balance             : 60000\n",
    "    Number of Products  : 2\n",
    "    Has Credit Card     : Yes    = [1]    <-- corresponds to\n",
    "    Is Active Member    : Yes    = [1]    <-- corresponds to\n",
    "    Estimated Salary    : 50000\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_array       = np.array([[0, 0, 600, 1, 40, 3, 60000, 2, 1, 1, 50000]])\n",
    "\n",
    "normal_array   = sc.transform(my_array)  # to normalize  \n",
    "\n",
    "new_prediction = classifier.predict(my_array)\n",
    "\n",
    "new_y_pred     = (new_prediction > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ True]]), array([[0.5228143]], dtype=float32))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_y_pred, new_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4 : Evaluating, Improving and Tuning the ANN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Evaluating the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "to fix this variance problem;\n",
    "k-Fold Cross Validation fix it by splitting the training set\n",
    "into 10 folds when K = 10, and most of the time K = 10\n",
    "and we train our model on 9-folds and we test it on the\n",
    "last remaining fold.\n",
    "there we take 10 different combination of 9-folds to train\n",
    "a model and 1-fold to test it.\n",
    "that means we can train the model and test the model\n",
    "on 10 combinations of training and test sets.\n",
    "And that will give us a much better idea of the model\n",
    "performance because, we take an average of different\n",
    "accuracies of the 10 evaluations and also compute\n",
    "the standart deviation to have a look at the variance.\n",
    "So eventually, our analysis will be much more relevant.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keras wrapper will wrap F-fold cross validation by Scikit-learn\n",
    "# in to the keras model\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def build_classifier():\n",
    "    classifier = Sequential()\n",
    "    classifier.add(Dense(6, activation = 'relu', input_shape = (11, )))\n",
    "    classifier.add(Dense(6, activation = 'relu'))\n",
    "    classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "    classifier.compile(optimizer = 'adam', \n",
    "                       loss      = 'binary_crossentropy',\n",
    "                       metrics   = ['accuracy']\n",
    "                      )\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_cv = KerasClassifier(build_fn   = build_classifier,\n",
    "                                batch_size = 10, \n",
    "                                epochs     = 100, \n",
    "                                verbose    = 0\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = cross_val_score(estimator = classifier_cv, \n",
    "                             X         = X_train, # the data to fit.\n",
    "                             y         = y_train, # the target to predict.\n",
    "                             cv        = 10,      # number of folds.\n",
    "                             n_jobs    = -1       # '-1' means all CPU's.\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.85374999, 0.86500001, 0.8775    , 0.86124998, 0.87      ,\n",
       "       0.85624999, 0.86000001, 0.85124999, 0.84875   , 0.86750001])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8611249983310699"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean            = accuracies.mean()\n",
    "mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.008559537963463334"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance        = accuracies.std()\n",
    "variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "we are in 'Low Bias Low Variance'.\n",
    "means, best accuracy low varince\n",
    "accuracy : % 86.1\n",
    "variance : % 0.85\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Improving the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dropout Regularization:\n",
    "it is the solution for overfitting in deep learning.\n",
    "Overfitting is when your model was trained too much\n",
    "on the training set, too much that it becomes much less\n",
    "performance on the test set and we can observe this\n",
    "when we have large difference of accuracies between\n",
    "training set and the test set.\n",
    "Generally, when overfitting happens, you have a much\n",
    "higher accuracy on the training set than the test set.\n",
    "And another way to detect overfitting is when you\n",
    "observe high variance when applying k-fold cv\n",
    "because indeed,  when it's overfitted on the training\n",
    "set, that is when your model learn too much and\n",
    "this may cause your model won't succeed on 'other' test\n",
    "sets because the correlations learned too much.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dropout works this way;\n",
    "At each iteration of the training, some neurons of your\n",
    "ANN are randomly disabled to prevent them for being too\n",
    "dependent on each other when they learn the correlations\n",
    "and therefore, by over-writing these neurons, the ANN\n",
    "learns several independent correlations in the data,\n",
    "because each time there is not the same configuration\n",
    "of the neurons.\n",
    "And the fact that we get these independent correlations\n",
    "of the data, thanks to the fact that the neurons work\n",
    "more independently, that prevents the neurons from learning\n",
    "too much and therefore that prevents overfitting.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropout Regularization to reduce overfitting if needed.\n",
    "from keras.layers import Dropout\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer with dropout\n",
    "classifier.add(Dense(6, activation = 'relu', input_shape = (11, )))\n",
    "classifier.add(Dropout(p = 0.1)) # p : fraction of the input units to drop.\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(6, activation = 'relu'))\n",
    "classifier.add(Dropout(p = 0.1))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "classifier.add(Dropout(p = 0.1))\n",
    "\n",
    "classifier.compile(optimizer = 'adam', \n",
    "                   loss      = 'binary_crossentropy',\n",
    "                   metrics   = ['accuracy']\n",
    "                  )\n",
    "\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.801"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "accuracy = (cm[0, 0] + cm[1, 1]) / np.sum(cm)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Tuning the ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def build_classifier(optimizer):\n",
    "    classifier = Sequential()\n",
    "    \n",
    "    classifier.add(Dense(6, activation = 'relu', input_shape = (11, )))\n",
    "    classifier.add(Dense(6, activation = 'relu'))\n",
    "    classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "    \n",
    "    classifier.compile(optimizer = optimizer, \n",
    "                       loss      = 'binary_crossentropy',\n",
    "                       metrics   = ['accuracy']\n",
    "                       )\n",
    "    return classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_cv   = KerasClassifier(build_fn = build_classifier)\n",
    "\n",
    "parameters      = {'batch_size' : [25, 32],\n",
    "                   'epochs'     : [100, 500],\n",
    "                   'optimizer'  : ['adam', 'rmsprop']\n",
    "                   }\n",
    "\n",
    "grid_search     = GridSearchCV(estimator  = classifier_cv,\n",
    "                               param_grid = parameters,\n",
    "                               scoring    = 'accuracy',\n",
    "                               cv         = 10,\n",
    "                               verbose    = 1\n",
    "                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search_cv   = grid_search.fit(X = X_train, y = y_train)\n",
    "best_parameters  = grid_search_cv.best_params_\n",
    "best_accuracy    = grid_search_cv.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# best_parameters = {'batch_size' : 32, 'epochs' : 100, 'optimizer' : 'rmsprop'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(6, activation = 'relu', input_shape = (11, )))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(6, activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy',\n",
    "                   metrics = ['accuracy'])\n",
    "\n",
    "classifier.fit(X_train, y_train, batch_size = 32, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tuned   = classifier.predict(X_test)\n",
    "y_pred_tuned   = (y_pred_tuned > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_tuned       = confusion_matrix(y_test, y_pred_tuned)\n",
    "accuracy_tuned = (cm_tuned[0, 0] + cm_tuned[1, 1]) / np.sum(cm_tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.862"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_tuned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
